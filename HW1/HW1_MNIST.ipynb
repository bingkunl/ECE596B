{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Extract MNIST data</h2>\n",
    "<p style=\"font-size:20px\">You can change the option of one_hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#get mnist data, with one_hot encoding\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "#suppress warnings\n",
    "tf.logging.set_verbosity(old_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = mnist.train.num_examples #55,000\n",
    "num_validation = mnist.validation.num_examples #5000\n",
    "num_test = mnist.test.num_examples #10,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\envname\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Training finished!\n",
      "learning rate: 0.01 activation functions: Sigmoid n_hidden_1: 500 n_hidden_2: 300 iterations: 2000 batch size: 128\n",
      "Testing Accuracy: 0.9399\n"
     ]
    }
   ],
   "source": [
    "#learning rate\n",
    "lr = 0.01\n",
    "#number of traning steps\n",
    "num_steps = 2000\n",
    "#number of batch_size\n",
    "batch_size = 128\n",
    "\n",
    "#network parameters\n",
    "n_hidden_1 = 500\n",
    "n_hidden_2 = 300\n",
    "num_input = 784\n",
    "num_classes = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.random_normal([num_input, n_hidden_1]),name='W1'),\n",
    "    'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]),name='W2'),\n",
    "    'Wout': tf.Variable(tf.random_normal([n_hidden_2, num_classes]),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros(shape=[n_hidden_1]),name='b1'),\n",
    "    'b2': tf.Variable(tf.zeros(shape=[n_hidden_2]),name='b2'),\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "}\n",
    "\n",
    "#define a neural net model\n",
    "def neural_net(x):\n",
    "    layer_1_out = tf.add(tf.matmul(x,weights['W1']),biases['b1'])\n",
    "    layer_1_out = tf.sigmoid(layer_1_out)\n",
    "    layer_2_out = tf.add(tf.matmul(layer_1_out,weights['W2']),biases['b2'])\n",
    "    layer_2_out = tf.sigmoid(layer_2_out)\n",
    "    out = tf.add(tf.matmul(layer_2_out,weights['Wout']),biases['bout'])\n",
    "    return out\n",
    "\n",
    "#predicted labels\n",
    "logits = neural_net(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        #fetch batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        #run optimization\n",
    "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    print(\"learning rate:\", lr, \"activation functions: Sigmoid\", \"n_hidden_1:\", n_hidden_1,\n",
    "      \"n_hidden_2:\", n_hidden_2, \"iterations:\", num_steps, \"batch size:\", batch_size)\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n",
      "learning rate: 0.01 activation functions: ReLU n_hidden_1: 500 n_hidden_2: 300 iterations: 2000 batch size: 128\n",
      "Testing Accuracy: 0.9591\n"
     ]
    }
   ],
   "source": [
    "#learning rate\n",
    "lr = 0.01\n",
    "#number of traning steps\n",
    "num_steps = 2000\n",
    "#number of batch_size\n",
    "batch_size = 128\n",
    "\n",
    "#network parameters\n",
    "n_hidden_1 = 500\n",
    "n_hidden_2 = 300\n",
    "num_input = 784\n",
    "num_classes = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.random_normal([num_input, n_hidden_1]),name='W1'),\n",
    "    'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]),name='W2'),\n",
    "    'Wout': tf.Variable(tf.random_normal([n_hidden_2, num_classes]),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros(shape=[n_hidden_1]),name='b1'),\n",
    "    'b2': tf.Variable(tf.zeros(shape=[n_hidden_2]),name='b2'),\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "}\n",
    "\n",
    "#define a neural net model\n",
    "def neural_net(x):\n",
    "    layer_1_out = tf.add(tf.matmul(x,weights['W1']),biases['b1'])\n",
    "    layer_1_out = tf.nn.relu(layer_1_out)\n",
    "    layer_2_out = tf.add(tf.matmul(layer_1_out,weights['W2']),biases['b2'])\n",
    "    layer_2_out = tf.nn.relu(layer_2_out)\n",
    "    out = tf.add(tf.matmul(layer_2_out,weights['Wout']),biases['bout'])\n",
    "    return out\n",
    "\n",
    "#predicted labels\n",
    "logits = neural_net(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        #fetch batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        #run optimization\n",
    "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    print(\"learning rate:\", lr, \"activation functions: ReLU\", \"n_hidden_1:\", n_hidden_1,\n",
    "      \"n_hidden_2:\", n_hidden_2, \"iterations:\", num_steps, \"batch size:\", batch_size)\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n",
      "learning rate: 0.01 activation functions: Sigmoid n_hidden_1: 300 n_hidden_2: 100 iterations: 20000 batch size: 128\n",
      "Testing Accuracy: 0.9684\n"
     ]
    }
   ],
   "source": [
    "#learning rate\n",
    "lr = 0.01\n",
    "#number of traning steps\n",
    "num_steps = 20000\n",
    "#number of batch_size\n",
    "batch_size = 128\n",
    "\n",
    "#network parameters\n",
    "n_hidden_1 = 300\n",
    "n_hidden_2 = 100\n",
    "num_input = 784\n",
    "num_classes = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.random_normal([num_input, n_hidden_1]),name='W1'),\n",
    "    'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]),name='W2'),\n",
    "    'Wout': tf.Variable(tf.random_normal([n_hidden_2, num_classes]),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros(shape=[n_hidden_1]),name='b1'),\n",
    "    'b2': tf.Variable(tf.zeros(shape=[n_hidden_2]),name='b2'),\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "}\n",
    "\n",
    "#define a neural net model\n",
    "def neural_net(x):\n",
    "    layer_1_out = tf.add(tf.matmul(x,weights['W1']),biases['b1'])\n",
    "    layer_1_out = tf.sigmoid(layer_1_out)\n",
    "    layer_2_out = tf.add(tf.matmul(layer_1_out,weights['W2']),biases['b2'])\n",
    "    layer_2_out = tf.sigmoid(layer_2_out)\n",
    "    out = tf.add(tf.matmul(layer_2_out,weights['Wout']),biases['bout'])\n",
    "    return out\n",
    "\n",
    "#predicted labels\n",
    "logits = neural_net(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        #fetch batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        #run optimization\n",
    "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    print(\"learning rate:\", lr, \"activation functions: Sigmoid\", \"n_hidden_1:\", n_hidden_1,\n",
    "      \"n_hidden_2:\", n_hidden_2, \"iterations:\", num_steps, \"batch size:\", batch_size)\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n",
      "learning rate: 0.01 activation functions: ReLU n_hidden_1: 300 n_hidden_2: 100 iterations: 20000 batch size: 128\n",
      "Testing Accuracy: 0.9533\n"
     ]
    }
   ],
   "source": [
    "#learning rate\n",
    "lr = 0.01\n",
    "#number of traning steps\n",
    "num_steps = 20000\n",
    "#number of batch_size\n",
    "batch_size = 128\n",
    "\n",
    "#network parameters\n",
    "n_hidden_1 = 300\n",
    "n_hidden_2 = 100\n",
    "num_input = 784\n",
    "num_classes = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.random_normal([num_input, n_hidden_1]),name='W1'),\n",
    "    'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]),name='W2'),\n",
    "    'Wout': tf.Variable(tf.random_normal([n_hidden_2, num_classes]),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros(shape=[n_hidden_1]),name='b1'),\n",
    "    'b2': tf.Variable(tf.zeros(shape=[n_hidden_2]),name='b2'),\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "}\n",
    "\n",
    "#define a neural net model\n",
    "def neural_net(x):\n",
    "    layer_1_out = tf.add(tf.matmul(x,weights['W1']),biases['b1'])\n",
    "    layer_1_out = tf.nn.relu(layer_1_out)\n",
    "    layer_2_out = tf.add(tf.matmul(layer_1_out,weights['W2']),biases['b2'])\n",
    "    layer_2_out = tf.nn.relu(layer_2_out)\n",
    "    out = tf.add(tf.matmul(layer_2_out,weights['Wout']),biases['bout'])\n",
    "    return out\n",
    "\n",
    "#predicted labels\n",
    "logits = neural_net(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        #fetch batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        #run optimization\n",
    "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    print(\"learning rate:\", lr, \"activation functions: ReLU\", \"n_hidden_1:\", n_hidden_1,\n",
    "      \"n_hidden_2:\", n_hidden_2, \"iterations:\", num_steps, \"batch size:\", batch_size)\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n",
      "learning rate: 0.01 activation functions: Sigmoid n_hidden_1: 500 n_hidden_2: 300 iterations: 20000 batch size: 64\n",
      "Testing Accuracy: 0.9619\n"
     ]
    }
   ],
   "source": [
    "#learning rate\n",
    "lr = 0.01\n",
    "#number of traning steps\n",
    "num_steps = 20000\n",
    "#number of batch_size\n",
    "batch_size = 64\n",
    "\n",
    "#network parameters\n",
    "n_hidden_1 = 500\n",
    "n_hidden_2 = 300\n",
    "num_input = 784\n",
    "num_classes = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.random_normal([num_input, n_hidden_1]),name='W1'),\n",
    "    'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]),name='W2'),\n",
    "    'Wout': tf.Variable(tf.random_normal([n_hidden_2, num_classes]),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros(shape=[n_hidden_1]),name='b1'),\n",
    "    'b2': tf.Variable(tf.zeros(shape=[n_hidden_2]),name='b2'),\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "}\n",
    "\n",
    "#define a neural net model\n",
    "def neural_net(x):\n",
    "    layer_1_out = tf.add(tf.matmul(x,weights['W1']),biases['b1'])\n",
    "    layer_1_out = tf.sigmoid(layer_1_out)\n",
    "    layer_2_out = tf.add(tf.matmul(layer_1_out,weights['W2']),biases['b2'])\n",
    "    layer_2_out = tf.sigmoid(layer_2_out)\n",
    "    out = tf.add(tf.matmul(layer_2_out,weights['Wout']),biases['bout'])\n",
    "    return out\n",
    "\n",
    "#predicted labels\n",
    "logits = neural_net(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        #fetch batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        #run optimization\n",
    "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    print(\"learning rate:\", lr, \"activation functions: Sigmoid\", \"n_hidden_1:\", n_hidden_1,\n",
    "      \"n_hidden_2:\", n_hidden_2, \"iterations:\", num_steps, \"batch size:\", batch_size)\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n",
      "learning rate: 0.01 activation functions: ReLU n_hidden_1: 500 n_hidden_2: 300 iterations: 20000 batch size: 64\n",
      "Testing Accuracy: 0.9434\n"
     ]
    }
   ],
   "source": [
    "#learning rate\n",
    "lr = 0.01\n",
    "#number of traning steps\n",
    "num_steps = 20000\n",
    "#number of batch_size\n",
    "batch_size = 64\n",
    "\n",
    "#network parameters\n",
    "n_hidden_1 = 500\n",
    "n_hidden_2 = 300\n",
    "num_input = 784\n",
    "num_classes = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.random_normal([num_input, n_hidden_1]),name='W1'),\n",
    "    'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]),name='W2'),\n",
    "    'Wout': tf.Variable(tf.random_normal([n_hidden_2, num_classes]),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros(shape=[n_hidden_1]),name='b1'),\n",
    "    'b2': tf.Variable(tf.zeros(shape=[n_hidden_2]),name='b2'),\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "}\n",
    "\n",
    "#define a neural net model\n",
    "def neural_net(x):\n",
    "    layer_1_out = tf.add(tf.matmul(x,weights['W1']),biases['b1'])\n",
    "    layer_1_out = tf.nn.relu(layer_1_out)\n",
    "    layer_2_out = tf.add(tf.matmul(layer_1_out,weights['W2']),biases['b2'])\n",
    "    layer_2_out = tf.nn.relu(layer_2_out)\n",
    "    out = tf.add(tf.matmul(layer_2_out,weights['Wout']),biases['bout'])\n",
    "    return out\n",
    "\n",
    "#predicted labels\n",
    "logits = neural_net(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        #fetch batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        #run optimization\n",
    "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    print(\"learning rate:\", lr, \"activation functions: ReLU\", \"n_hidden_1:\", n_hidden_1,\n",
    "      \"n_hidden_2:\", n_hidden_2, \"iterations:\", num_steps, \"batch size:\", batch_size)\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n",
      "learning rate: 0.001 activation functions: Sigmoid n_hidden_1: 500 n_hidden_2: 300 iterations: 2000 batch size: 128\n",
      "Testing Accuracy: 0.9003\n"
     ]
    }
   ],
   "source": [
    "#learning rate\n",
    "lr = 0.001\n",
    "#number of traning steps\n",
    "num_steps = 2000\n",
    "#number of batch_size\n",
    "batch_size = 128\n",
    "\n",
    "#network parameters\n",
    "n_hidden_1 = 500\n",
    "n_hidden_2 = 300\n",
    "num_input = 784\n",
    "num_classes = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.random_normal([num_input, n_hidden_1]),name='W1'),\n",
    "    'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]),name='W2'),\n",
    "    'Wout': tf.Variable(tf.random_normal([n_hidden_2, num_classes]),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros(shape=[n_hidden_1]),name='b1'),\n",
    "    'b2': tf.Variable(tf.zeros(shape=[n_hidden_2]),name='b2'),\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "}\n",
    "\n",
    "#define a neural net model\n",
    "def neural_net(x):\n",
    "    layer_1_out = tf.add(tf.matmul(x,weights['W1']),biases['b1'])\n",
    "    layer_1_out = tf.sigmoid(layer_1_out)\n",
    "    layer_2_out = tf.add(tf.matmul(layer_1_out,weights['W2']),biases['b2'])\n",
    "    layer_2_out = tf.sigmoid(layer_2_out)\n",
    "    out = tf.add(tf.matmul(layer_2_out,weights['Wout']),biases['bout'])\n",
    "    return out\n",
    "\n",
    "#predicted labels\n",
    "logits = neural_net(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        #fetch batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        #run optimization\n",
    "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    print(\"learning rate:\", lr, \"activation functions: Sigmoid\", \"n_hidden_1:\", n_hidden_1,\n",
    "      \"n_hidden_2:\", n_hidden_2, \"iterations:\", num_steps, \"batch size:\", batch_size)\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n",
      "learning rate: 0.001 activation functions: ReLU n_hidden_1: 500 n_hidden_2: 300 iterations: 2000 batch size: 128\n",
      "Testing Accuracy: 0.9298\n"
     ]
    }
   ],
   "source": [
    "#learning rate\n",
    "lr = 0.001\n",
    "#number of traning steps\n",
    "num_steps = 2000\n",
    "#number of batch_size\n",
    "batch_size = 128\n",
    "\n",
    "#network parameters\n",
    "n_hidden_1 = 500\n",
    "n_hidden_2 = 300\n",
    "num_input = 784\n",
    "num_classes = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.random_normal([num_input, n_hidden_1]),name='W1'),\n",
    "    'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]),name='W2'),\n",
    "    'Wout': tf.Variable(tf.random_normal([n_hidden_2, num_classes]),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros(shape=[n_hidden_1]),name='b1'),\n",
    "    'b2': tf.Variable(tf.zeros(shape=[n_hidden_2]),name='b2'),\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "}\n",
    "\n",
    "#define a neural net model\n",
    "def neural_net(x):\n",
    "    layer_1_out = tf.add(tf.matmul(x,weights['W1']),biases['b1'])\n",
    "    layer_1_out = tf.nn.relu(layer_1_out)\n",
    "    layer_2_out = tf.add(tf.matmul(layer_1_out,weights['W2']),biases['b2'])\n",
    "    layer_2_out = tf.nn.relu(layer_2_out)\n",
    "    out = tf.add(tf.matmul(layer_2_out,weights['Wout']),biases['bout'])\n",
    "    return out\n",
    "\n",
    "#predicted labels\n",
    "logits = neural_net(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        #fetch batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        #run optimization\n",
    "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    print(\"learning rate:\", lr, \"activation functions: ReLU\", \"n_hidden_1:\", n_hidden_1,\n",
    "      \"n_hidden_2:\", n_hidden_2, \"iterations:\", num_steps, \"batch size:\", batch_size)\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n",
      "learning rate: 0.001 activation functions: Sigmoid n_hidden_1: 300 n_hidden_2: 100 iterations: 20000 batch size: 128\n",
      "Testing Accuracy: 0.9584\n"
     ]
    }
   ],
   "source": [
    "#learning rate\n",
    "lr = 0.001\n",
    "#number of traning steps\n",
    "num_steps = 20000\n",
    "#number of batch_size\n",
    "batch_size = 128\n",
    "\n",
    "#network parameters\n",
    "n_hidden_1 = 300\n",
    "n_hidden_2 = 100\n",
    "num_input = 784\n",
    "num_classes = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.random_normal([num_input, n_hidden_1]),name='W1'),\n",
    "    'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]),name='W2'),\n",
    "    'Wout': tf.Variable(tf.random_normal([n_hidden_2, num_classes]),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros(shape=[n_hidden_1]),name='b1'),\n",
    "    'b2': tf.Variable(tf.zeros(shape=[n_hidden_2]),name='b2'),\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "}\n",
    "\n",
    "#define a neural net model\n",
    "def neural_net(x):\n",
    "    layer_1_out = tf.add(tf.matmul(x,weights['W1']),biases['b1'])\n",
    "    layer_1_out = tf.sigmoid(layer_1_out)\n",
    "    layer_2_out = tf.add(tf.matmul(layer_1_out,weights['W2']),biases['b2'])\n",
    "    layer_2_out = tf.sigmoid(layer_2_out)\n",
    "    out = tf.add(tf.matmul(layer_2_out,weights['Wout']),biases['bout'])\n",
    "    return out\n",
    "\n",
    "#predicted labels\n",
    "logits = neural_net(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        #fetch batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        #run optimization\n",
    "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    print(\"learning rate:\", lr, \"activation functions: Sigmoid\", \"n_hidden_1:\", n_hidden_1,\n",
    "      \"n_hidden_2:\", n_hidden_2, \"iterations:\", num_steps, \"batch size:\", batch_size)\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n",
      "learning rate: 0.001 activation functions: ReLU n_hidden_1: 300 n_hidden_2: 100 iterations: 20000 batch size: 128\n",
      "Testing Accuracy: 0.9568\n"
     ]
    }
   ],
   "source": [
    "#learning rate\n",
    "lr = 0.001\n",
    "#number of traning steps\n",
    "num_steps = 20000\n",
    "#number of batch_size\n",
    "batch_size = 128\n",
    "\n",
    "#network parameters\n",
    "n_hidden_1 = 300\n",
    "n_hidden_2 = 100\n",
    "num_input = 784\n",
    "num_classes = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.random_normal([num_input, n_hidden_1]),name='W1'),\n",
    "    'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]),name='W2'),\n",
    "    'Wout': tf.Variable(tf.random_normal([n_hidden_2, num_classes]),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros(shape=[n_hidden_1]),name='b1'),\n",
    "    'b2': tf.Variable(tf.zeros(shape=[n_hidden_2]),name='b2'),\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "}\n",
    "\n",
    "#define a neural net model\n",
    "def neural_net(x):\n",
    "    layer_1_out = tf.add(tf.matmul(x,weights['W1']),biases['b1'])\n",
    "    layer_1_out = tf.nn.relu(layer_1_out)\n",
    "    layer_2_out = tf.add(tf.matmul(layer_1_out,weights['W2']),biases['b2'])\n",
    "    layer_2_out = tf.nn.relu(layer_2_out)\n",
    "    out = tf.add(tf.matmul(layer_2_out,weights['Wout']),biases['bout'])\n",
    "    return out\n",
    "\n",
    "#predicted labels\n",
    "logits = neural_net(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        #fetch batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        #run optimization\n",
    "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    print(\"learning rate:\", lr, \"activation functions: ReLU\", \"n_hidden_1:\", n_hidden_1,\n",
    "      \"n_hidden_2:\", n_hidden_2, \"iterations:\", num_steps, \"batch size:\", batch_size)\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n",
      "learning rate: 0.001 activation functions: Sigmoid n_hidden_1: 500 n_hidden_2: 300 iterations: 20000 batch size: 64\n",
      "Testing Accuracy: 0.9554\n"
     ]
    }
   ],
   "source": [
    "#learning rate\n",
    "lr = 0.001\n",
    "#number of traning steps\n",
    "num_steps = 20000\n",
    "#number of batch_size\n",
    "batch_size = 64\n",
    "\n",
    "#network parameters\n",
    "n_hidden_1 = 500\n",
    "n_hidden_2 = 300\n",
    "num_input = 784\n",
    "num_classes = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.random_normal([num_input, n_hidden_1]),name='W1'),\n",
    "    'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]),name='W2'),\n",
    "    'Wout': tf.Variable(tf.random_normal([n_hidden_2, num_classes]),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros(shape=[n_hidden_1]),name='b1'),\n",
    "    'b2': tf.Variable(tf.zeros(shape=[n_hidden_2]),name='b2'),\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "}\n",
    "\n",
    "#define a neural net model\n",
    "def neural_net(x):\n",
    "    layer_1_out = tf.add(tf.matmul(x,weights['W1']),biases['b1'])\n",
    "    layer_1_out = tf.sigmoid(layer_1_out)\n",
    "    layer_2_out = tf.add(tf.matmul(layer_1_out,weights['W2']),biases['b2'])\n",
    "    layer_2_out = tf.sigmoid(layer_2_out)\n",
    "    out = tf.add(tf.matmul(layer_2_out,weights['Wout']),biases['bout'])\n",
    "    return out\n",
    "\n",
    "#predicted labels\n",
    "logits = neural_net(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        #fetch batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        #run optimization\n",
    "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    print(\"learning rate:\", lr, \"activation functions: Sigmoid\", \"n_hidden_1:\", n_hidden_1,\n",
    "      \"n_hidden_2:\", n_hidden_2, \"iterations:\", num_steps, \"batch size:\", batch_size)\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n",
      "learning rate: 0.001 activation functions: ReLU n_hidden_1: 500 n_hidden_2: 300 iterations: 20000 batch size: 64\n",
      "Testing Accuracy: 0.9614\n"
     ]
    }
   ],
   "source": [
    "#learning rate\n",
    "lr = 0.001\n",
    "#number of traning steps\n",
    "num_steps = 20000\n",
    "#number of batch_size\n",
    "batch_size = 64\n",
    "\n",
    "#network parameters\n",
    "n_hidden_1 = 500\n",
    "n_hidden_2 = 300\n",
    "num_input = 784\n",
    "num_classes = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.random_normal([num_input, n_hidden_1]),name='W1'),\n",
    "    'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]),name='W2'),\n",
    "    'Wout': tf.Variable(tf.random_normal([n_hidden_2, num_classes]),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros(shape=[n_hidden_1]),name='b1'),\n",
    "    'b2': tf.Variable(tf.zeros(shape=[n_hidden_2]),name='b2'),\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "}\n",
    "\n",
    "#define a neural net model\n",
    "def neural_net(x):\n",
    "    layer_1_out = tf.add(tf.matmul(x,weights['W1']),biases['b1'])\n",
    "    layer_1_out = tf.nn.relu(layer_1_out)\n",
    "    layer_2_out = tf.add(tf.matmul(layer_1_out,weights['W2']),biases['b2'])\n",
    "    layer_2_out = tf.nn.relu(layer_2_out)\n",
    "    out = tf.add(tf.matmul(layer_2_out,weights['Wout']),biases['bout'])\n",
    "    return out\n",
    "\n",
    "#predicted labels\n",
    "logits = neural_net(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        #fetch batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        #run optimization\n",
    "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    print(\"learning rate:\", lr, \"activation functions: ReLU\", \"n_hidden_1:\", n_hidden_1,\n",
    "      \"n_hidden_2:\", n_hidden_2, \"iterations:\", num_steps, \"batch size:\", batch_size)\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Your results</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| learning rate | activation functions | n_hidden_1 | n_hidden_2 | iterations | batch size | accuracy |\n",
    "|---------------|----------------------|------------|------------|------------|------------|----------|\n",
    "| 0.01          | Sigmoid              | 500        | 300        | 2000       | 128        | 0.9399   |\n",
    "| 0.01          | ReLU                 | 500        | 300        | 2000       | 128        | 0.9591   |\n",
    "| 0.01          | Sigmoid              | 300        | 100        | 20000      | 128        | 0.9684   |\n",
    "| 0.01          | ReLU                 | 300        | 100        | 20000      | 128        | 0.9533   |\n",
    "| 0.01          | Sigmoid              | 500        | 300        | 20000      | 64         | 0.9619   |\n",
    "| 0.01          | ReLU                 | 500        | 300        | 20000      | 64         | 0.9434   |\n",
    "| 0.001         | Sigmoid              | 500        | 300        | 2000       | 128        | 0.9003   |\n",
    "| 0.001         | Relu                 | 500        | 300        | 2000       | 128        | 0.9298   |\n",
    "| 0.001         | Sigmoid              | 300        | 100        | 20000      | 128        | 0.9584   |\n",
    "| 0.001         | ReLU                 | 300        | 100        | 20000      | 128        | 0.9568   |\n",
    "| 0.001         | Sigmoid              | 500        | 300        | 20000      | 64         | 0.9554   |\n",
    "| 0.001         | ReLU                 | 500        | 300        | 20000      | 64         | 0.9614   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of GradientDescent, I used Adam Optimizer which converges faster. So I choose learning rate 0.01 and 0.001 for my test. Comparing the first six results with the last six resuls, we could see that a larger learning rate has better performance in general. But we also notice that when we use ReLU as the activation function, it performs better with a lower learning rate, as we could see in the last test. I added activation functions and increased the number of iterations which resulted in a great improvement on accuracy, from 0.8 to 0.9 and above. For the batch size and the number of neurons, I don't think it will cause huge difference as we could see in the graph that the accuracy is quite similar. I didn't try to change the number of layers so I can't say the impact. From my persepective, all tests with accuracy above the 96% are trained with 20000 iterations. I think the number of iterations contributes the most to improve the performance, but I also think that this improvement will have a limitation which means that the number of iterations is too large and the outcome remain the same.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
