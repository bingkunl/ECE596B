{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import timeit\n",
    "from load_cifar import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "validation_data, validation_labels = load_preprocessed_validation_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Hyper-perparmeter</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "n_hidden_1 = 500\n",
    "n_hidden_2 = 300\n",
    "n_hidden_3 = 200\n",
    "num_input = 3072\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Placeholder</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Neural Network Architecture</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = tf.layers.dense(X, n_hidden_1, activation=tf.nn.relu)\n",
    "# layer_1_bn = tf.layers.batch_normalization(layer_1)\n",
    "layer_2 = tf.layers.dense(layer_1, n_hidden_2, activation=tf.nn.relu)\n",
    "# layer_2_bn = tf.layers.batch_normalization(layer_2)\n",
    "layer_3 = tf.layers.dense(layer_2, n_hidden_3, activation=tf.nn.relu)\n",
    "# layer_3_bn = tf.layers.batch_normalization(layer_3)\n",
    "logits = tf.layers.dense(layer_3, num_classes, activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define cost andoptimization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "\n",
    "# define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "# compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(tf.nn.softmax(logits),1),tf.argmax(Y,1))\n",
    "\n",
    "# compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training and testing</h1>\n",
    "<h2>1.Print out validation accuracy after each training poch</h2>\n",
    "<h2>2.Print out training time you spend on each epoch</h2>\n",
    "<h2>3.Print out testing accuracy in the end</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0   Time:2.944   Current validation accuracy: 0.3822\n",
      "Epoch:1   Time:2.638   Current validation accuracy: 0.4168\n",
      "Epoch:2   Time:2.635   Current validation accuracy: 0.438\n",
      "Epoch:3   Time:2.634   Current validation accuracy: 0.4522\n",
      "Epoch:4   Time:2.632   Current validation accuracy: 0.4654\n",
      "Epoch:5   Time:2.637   Current validation accuracy: 0.4722\n",
      "Epoch:6   Time:2.637   Current validation accuracy: 0.4844\n",
      "Epoch:7   Time:2.625   Current validation accuracy: 0.4848\n",
      "Epoch:8   Time:2.622   Current validation accuracy: 0.4912\n",
      "Epoch:9   Time:2.623   Current validation accuracy: 0.5002\n",
      "Test accuracy: \n",
      "0.5049\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_epochs):\n",
    "        s = time.time()\n",
    "        batch_id = 1\n",
    "        while batch_id < 6:\n",
    "            training_data = load_preprocessed_training_batch(batch_id, batch_size)\n",
    "            for data, labels in training_data:\n",
    "                sess.run(train_op, feed_dict = {X: data, Y: labels})\n",
    "            batch_id = batch_id + 1\n",
    "        val_acc = sess.run(accuracy, feed_dict={X: validation_data, Y: validation_labels})\n",
    "        f = time.time()\n",
    "        print('Epoch:' + str(i) + '   Time:{:.3f}'.format(f - s) + '   Current validation accuracy: ' + str(val_acc))\n",
    "    test_data, test_labels = load_all_test_batch()\n",
    "    test_acc = sess.run(accuracy, feed_dict = {X: test_data, Y: test_labels})\n",
    "    print('Test accuracy: ')\n",
    "    print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've used several kinds of fully connected neural network with different number of neurals in each layer. I found that there is always a limitation of a certain kind of network. For example, when I use two layers with 500 plus 300 neurals, the validation accuracy will stuck at about 0.47 unless increasing the number of epoch. I used 500 plus 300 neurals in homework 1 and the effect isn't bad. At first, I revised my network based more on the validation accuracy, for one reason is that accuracy, to some extent, could tell whether the network performed better or not, for another reason is that I didn't implement the test part by that time. I add one more layer with 100 neruals first and it couldn't break the bound, so I increase the number of neurals to 200. Activation function ReLu has a more better effect compared with common sigmoid and tanh, I used it for all three layers. As for the batch size, I modified it with the learning rate. I found that a big batch size needed a smaller learning rate. I've tried 32, 64, 128 batch size and 128 performs better, so I decide to adjust the learning rate with the 128 batch size. For the initialization, regularization, dropout and normalization part, they are somehow to avoid the overfitting situation. For my network here, I implement normalization which has tiny influences. I think the whole architecture of network is more important. The cost and optimization part is just like those in homework 1. Like the result shown above, I also try 20 epoch and the validation rate fluctuated around 0.52 which I think it may the limitation of this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
