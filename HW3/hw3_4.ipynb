{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import text_utils as txt\n",
    "from six.moves import cPickle\n",
    "from tensorflow.contrib import rnn\n",
    "from char_rnn_model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define directories, hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_v = tf.logging.get_verbosity()\n",
    "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "# tf.logging.set_verbosity(old_v)\n",
    "\n",
    "data_dir = './'\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "seq_len = 32 \n",
    "num_batches = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data using TextLoader object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = txt.TextLoader(data_dir, batch_size, seq_len)\n",
    "train_data, val_data, char = loader.loadData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = txt.fetch_batch(train_data, batch_size, seq_len)\n",
    "model = Model(batch_size, seq_len, loader.vocab_size, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 200  Val_Loss: 3.3729 \n",
      "Batch: 400  Val_Loss: 2.8699 \n",
      "Batch: 600  Val_Loss: 2.8005 \n",
      "Batch: 800  Val_Loss: 2.6972 \n",
      "Batch: 1000  Val_Loss: 2.6723 \n",
      "Batch: 1200  Val_Loss: 2.6787 \n",
      "Batch: 1400  Val_Loss: 2.6052 \n",
      "Batch: 1600  Val_Loss: 2.5421 \n",
      "Batch: 1800  Val_Loss: 2.5153 \n",
      "Batch: 2000  Val_Loss: 2.5654 \n",
      "Batch: 2200  Val_Loss: 2.5524 \n",
      "Batch: 2400  Val_Loss: 2.4927 \n",
      "Batch: 2600  Val_Loss: 2.4932 \n",
      "Batch: 2800  Val_Loss: 2.5398 \n",
      "Batch: 3000  Val_Loss: 2.5880 \n",
      "Batch: 3200  Val_Loss: 2.3152 \n",
      "Batch: 3400  Val_Loss: 2.4648 \n",
      "Batch: 3600  Val_Loss: 2.4652 \n",
      "Batch: 3800  Val_Loss: 2.5004 \n",
      "Batch: 4000  Val_Loss: 2.3978 \n",
      "Batch: 4200  Val_Loss: 2.4089 \n",
      "Batch: 4400  Val_Loss: 2.4082 \n",
      "Batch: 4600  Val_Loss: 2.4443 \n",
      "Batch: 4800  Val_Loss: 2.3474 \n",
      "Batch: 5000  Val_Loss: 2.3359 \n",
      "Batch: 5200  Val_Loss: 2.3938 \n",
      "Batch: 5400  Val_Loss: 2.2740 \n",
      "Batch: 5600  Val_Loss: 2.3181 \n",
      "Batch: 5800  Val_Loss: 2.3859 \n",
      "Batch: 6000  Val_Loss: 2.3311 \n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\envname\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Batch: 6200  Val_Loss: 2.3145 \n",
      "Batch: 6400  Val_Loss: 2.4059 \n",
      "Batch: 6600  Val_Loss: 2.3217 \n",
      "Batch: 6800  Val_Loss: 2.2635 \n",
      "Batch: 7000  Val_Loss: 2.2684 \n",
      "Batch: 7200  Val_Loss: 2.3277 \n",
      "Batch: 7400  Val_Loss: 2.2748 \n",
      "Batch: 7600  Val_Loss: 2.3169 \n",
      "Batch: 7800  Val_Loss: 2.3181 \n",
      "Batch: 8000  Val_Loss: 2.2399 \n",
      "Batch: 8200  Val_Loss: 2.3023 \n",
      "Batch: 8400  Val_Loss: 2.3041 \n",
      "Batch: 8600  Val_Loss: 2.2824 \n",
      "Batch: 8800  Val_Loss: 2.2410 \n",
      "Batch: 9000  Val_Loss: 2.2202 \n",
      "Batch: 9200  Val_Loss: 2.3153 \n",
      "Batch: 9400  Val_Loss: 2.2726 \n",
      "Batch: 9600  Val_Loss: 2.2432 \n",
      "Batch: 9800  Val_Loss: 2.2497 \n",
      "Batch: 10000  Val_Loss: 2.1883 \n",
      "Batch: 10200  Val_Loss: 2.1938 \n",
      "Batch: 10400  Val_Loss: 2.2507 \n",
      "Batch: 10600  Val_Loss: 2.1929 \n",
      "Batch: 10800  Val_Loss: 2.2706 \n",
      "Batch: 11000  Val_Loss: 2.1714 \n",
      "Batch: 11200  Val_Loss: 2.1728 \n",
      "Batch: 11400  Val_Loss: 2.3003 \n",
      "Batch: 11600  Val_Loss: 2.2416 \n",
      "Batch: 11800  Val_Loss: 2.3158 \n",
      "Batch: 12000  Val_Loss: 2.2854 \n",
      "Batch: 12200  Val_Loss: 2.3100 \n",
      "Batch: 12400  Val_Loss: 2.2670 \n",
      "Batch: 12600  Val_Loss: 2.1219 \n",
      "Batch: 12800  Val_Loss: 2.1657 \n",
      "Batch: 13000  Val_Loss: 2.2123 \n",
      "Batch: 13200  Val_Loss: 2.1855 \n",
      "Batch: 13400  Val_Loss: 2.1576 \n",
      "Batch: 13600  Val_Loss: 2.2774 \n",
      "Batch: 13800  Val_Loss: 2.1931 \n",
      "Batch: 14000  Val_Loss: 2.2161 \n",
      "Batch: 14200  Val_Loss: 2.1465 \n",
      "Batch: 14400  Val_Loss: 2.1998 \n",
      "Batch: 14600  Val_Loss: 2.2290 \n",
      "Batch: 14800  Val_Loss: 2.2146 \n",
      "Batch: 15000  Val_Loss: 2.1499 \n",
      "Batch: 15200  Val_Loss: 2.1174 \n",
      "Batch: 15400  Val_Loss: 2.1502 \n",
      "Batch: 15600  Val_Loss: 2.2258 \n",
      "Batch: 15800  Val_Loss: 2.1405 \n",
      "Batch: 16000  Val_Loss: 2.1061 \n",
      "Batch: 16200  Val_Loss: 2.1782 \n",
      "Batch: 16400  Val_Loss: 2.1336 \n",
      "Batch: 16600  Val_Loss: 2.1539 \n",
      "Batch: 16800  Val_Loss: 2.1739 \n",
      "Batch: 17000  Val_Loss: 2.1656 \n",
      "Batch: 17200  Val_Loss: 2.2305 \n",
      "Batch: 17400  Val_Loss: 2.2105 \n",
      "Batch: 17600  Val_Loss: 2.0470 \n",
      "Batch: 17800  Val_Loss: 2.1087 \n",
      "Batch: 18000  Val_Loss: 2.1374 \n",
      "Batch: 18200  Val_Loss: 2.1739 \n",
      "Batch: 18400  Val_Loss: 2.1032 \n",
      "Batch: 18600  Val_Loss: 2.1839 \n",
      "Batch: 18800  Val_Loss: 2.1315 \n",
      "Batch: 19000  Val_Loss: 2.1193 \n",
      "Batch: 19200  Val_Loss: 2.1332 \n",
      "Batch: 19400  Val_Loss: 2.1247 \n",
      "Batch: 19600  Val_Loss: 2.2141 \n",
      "Batch: 19800  Val_Loss: 2.1266 \n",
      "Batch: 20000  Val_Loss: 2.1554 \n",
      "Batch: 20200  Val_Loss: 2.2680 \n",
      "Batch: 20400  Val_Loss: 2.1229 \n",
      "Batch: 20600  Val_Loss: 2.2430 \n",
      "Batch: 20800  Val_Loss: 2.1705 \n",
      "Batch: 21000  Val_Loss: 2.1828 \n",
      "Batch: 21200  Val_Loss: 2.1835 \n",
      "Batch: 21400  Val_Loss: 2.1353 \n",
      "Batch: 21600  Val_Loss: 2.0705 \n",
      "Batch: 21800  Val_Loss: 2.0695 \n",
      "Batch: 22000  Val_Loss: 2.0913 \n",
      "Batch: 22200  Val_Loss: 2.1494 \n",
      "Batch: 22400  Val_Loss: 2.1531 \n",
      "Batch: 22600  Val_Loss: 2.0062 \n",
      "Batch: 22800  Val_Loss: 2.1442 \n",
      "Batch: 23000  Val_Loss: 2.1734 \n",
      "Batch: 23200  Val_Loss: 2.1130 \n",
      "Batch: 23400  Val_Loss: 2.1777 \n",
      "Batch: 23600  Val_Loss: 2.2205 \n",
      "Batch: 23800  Val_Loss: 2.1332 \n",
      "Batch: 24000  Val_Loss: 2.1190 \n",
      "Batch: 24200  Val_Loss: 2.1540 \n",
      "Batch: 24400  Val_Loss: 2.0509 \n",
      "Batch: 24600  Val_Loss: 2.0800 \n",
      "Batch: 24800  Val_Loss: 2.1703 \n",
      "Batch: 25000  Val_Loss: 2.0404 \n",
      "Batch: 25200  Val_Loss: 2.0854 \n",
      "Batch: 25400  Val_Loss: 2.2099 \n",
      "Batch: 25600  Val_Loss: 2.3147 \n",
      "Batch: 25800  Val_Loss: 2.0677 \n",
      "Batch: 26000  Val_Loss: 2.1590 \n",
      "Batch: 26200  Val_Loss: 2.1965 \n",
      "Batch: 26400  Val_Loss: 2.1397 \n",
      "Batch: 26600  Val_Loss: 2.1791 \n",
      "Batch: 26800  Val_Loss: 2.0994 \n",
      "Batch: 27000  Val_Loss: 2.0894 \n",
      "Batch: 27200  Val_Loss: 2.2467 \n",
      "Batch: 27400  Val_Loss: 2.1388 \n",
      "Batch: 27600  Val_Loss: 2.0665 \n",
      "Batch: 27800  Val_Loss: 2.0118 \n",
      "Batch: 28000  Val_Loss: 2.1055 \n",
      "Batch: 28200  Val_Loss: 2.0975 \n",
      "Batch: 28400  Val_Loss: 2.0807 \n",
      "Batch: 28600  Val_Loss: 2.1541 \n",
      "Batch: 28800  Val_Loss: 2.0802 \n",
      "Batch: 29000  Val_Loss: 2.2085 \n",
      "Batch: 29200  Val_Loss: 2.1374 \n",
      "Batch: 29400  Val_Loss: 2.0907 \n",
      "Batch: 29600  Val_Loss: 2.0809 \n",
      "Batch: 29800  Val_Loss: 2.0911 \n",
      "Batch: 30000  Val_Loss: 2.1380 \n",
      "Batch: 30200  Val_Loss: 2.0930 \n",
      "Batch: 30400  Val_Loss: 2.0541 \n",
      "Batch: 30600  Val_Loss: 2.0988 \n",
      "Batch: 30800  Val_Loss: 2.1895 \n",
      "Batch: 31000  Val_Loss: 2.0446 \n",
      "Batch: 31200  Val_Loss: 2.0847 \n",
      "Batch: 31400  Val_Loss: 2.0607 \n",
      "Batch: 31600  Val_Loss: 2.0493 \n",
      "Batch: 31800  Val_Loss: 2.1416 \n",
      "Batch: 32000  Val_Loss: 2.0884 \n",
      "Batch: 32200  Val_Loss: 2.0532 \n",
      "Batch: 32400  Val_Loss: 2.0950 \n",
      "Batch: 32600  Val_Loss: 2.0080 \n",
      "Batch: 32800  Val_Loss: 2.1512 \n",
      "Batch: 33000  Val_Loss: 2.0135 \n",
      "Batch: 33200  Val_Loss: 2.1346 \n",
      "Batch: 33400  Val_Loss: 2.0951 \n",
      "Batch: 33600  Val_Loss: 2.1473 \n",
      "Batch: 33800  Val_Loss: 2.0753 \n",
      "Batch: 34000  Val_Loss: 2.1068 \n",
      "Batch: 34200  Val_Loss: 2.1322 \n",
      "Batch: 34400  Val_Loss: 2.0483 \n",
      "Batch: 34600  Val_Loss: 1.9566 \n",
      "Batch: 34800  Val_Loss: 2.1606 \n",
      "Batch: 35000  Val_Loss: 1.9718 \n",
      "Batch: 35200  Val_Loss: 1.9826 \n",
      "Batch: 35400  Val_Loss: 2.1250 \n",
      "Batch: 35600  Val_Loss: 2.0448 \n",
      "Batch: 35800  Val_Loss: 2.1278 \n",
      "Batch: 36000  Val_Loss: 2.1093 \n",
      "Batch: 36200  Val_Loss: 2.0476 \n",
      "Batch: 36400  Val_Loss: 2.0639 \n",
      "Batch: 36600  Val_Loss: 2.1130 \n",
      "Batch: 36800  Val_Loss: 2.0261 \n",
      "Batch: 37000  Val_Loss: 2.0692 \n",
      "Batch: 37200  Val_Loss: 2.1073 \n",
      "Batch: 37400  Val_Loss: 2.1180 \n",
      "Batch: 37600  Val_Loss: 2.0309 \n",
      "Batch: 37800  Val_Loss: 2.0581 \n",
      "Batch: 38000  Val_Loss: 2.1253 \n",
      "Batch: 38200  Val_Loss: 2.0218 \n",
      "Batch: 38400  Val_Loss: 2.2087 \n",
      "Batch: 38600  Val_Loss: 1.9571 \n",
      "Batch: 38800  Val_Loss: 2.0771 \n",
      "Batch: 39000  Val_Loss: 2.0913 \n",
      "Batch: 39200  Val_Loss: 2.0454 \n",
      "Batch: 39400  Val_Loss: 2.0244 \n",
      "Batch: 39600  Val_Loss: 2.0087 \n",
      "Batch: 39800  Val_Loss: 1.9821 \n",
      "Batch: 40000  Val_Loss: 2.0396 \n",
      "Batch: 40200  Val_Loss: 1.9938 \n",
      "Batch: 40400  Val_Loss: 2.0788 \n",
      "Batch: 40600  Val_Loss: 2.1806 \n",
      "Batch: 40800  Val_Loss: 2.0988 \n",
      "Batch: 41000  Val_Loss: 2.1075 \n",
      "Batch: 41200  Val_Loss: 2.0707 \n",
      "Batch: 41400  Val_Loss: 2.0702 \n",
      "Batch: 41600  Val_Loss: 2.0744 \n",
      "Batch: 41800  Val_Loss: 2.1100 \n",
      "Batch: 42000  Val_Loss: 2.1138 \n",
      "Batch: 42200  Val_Loss: 2.0656 \n",
      "Batch: 42400  Val_Loss: 2.1199 \n",
      "Batch: 42600  Val_Loss: 2.0690 \n",
      "Batch: 42800  Val_Loss: 1.9322 \n",
      "Batch: 43000  Val_Loss: 2.1424 \n",
      "Batch: 43200  Val_Loss: 1.9798 \n",
      "Batch: 43400  Val_Loss: 2.0511 \n",
      "Batch: 43600  Val_Loss: 1.9949 \n",
      "Batch: 43800  Val_Loss: 1.9436 \n",
      "Batch: 44000  Val_Loss: 1.9853 \n",
      "Batch: 44200  Val_Loss: 2.0743 \n",
      "Batch: 44400  Val_Loss: 2.0311 \n",
      "Batch: 44600  Val_Loss: 2.0834 \n",
      "Batch: 44800  Val_Loss: 2.0728 \n",
      "Batch: 45000  Val_Loss: 2.1332 \n",
      "Batch: 45200  Val_Loss: 2.0458 \n",
      "Batch: 45400  Val_Loss: 2.0875 \n",
      "Batch: 45600  Val_Loss: 1.9836 \n",
      "Batch: 45800  Val_Loss: 2.0499 \n",
      "Batch: 46000  Val_Loss: 2.0228 \n",
      "Batch: 46200  Val_Loss: 2.0927 \n",
      "Batch: 46400  Val_Loss: 2.0892 \n",
      "Batch: 46600  Val_Loss: 2.0189 \n",
      "Batch: 46800  Val_Loss: 2.0680 \n",
      "Batch: 47000  Val_Loss: 1.9399 \n",
      "Batch: 47200  Val_Loss: 2.0746 \n",
      "Batch: 47400  Val_Loss: 2.0792 \n",
      "Batch: 47600  Val_Loss: 2.0600 \n",
      "Batch: 47800  Val_Loss: 2.0391 \n",
      "Batch: 48000  Val_Loss: 2.0544 \n",
      "Batch: 48200  Val_Loss: 2.1110 \n",
      "Batch: 48400  Val_Loss: 2.0062 \n",
      "Batch: 48600  Val_Loss: 2.1475 \n",
      "Batch: 48800  Val_Loss: 2.1253 \n",
      "Batch: 49000  Val_Loss: 2.0024 \n",
      "Batch: 49200  Val_Loss: 2.0705 \n",
      "Batch: 49400  Val_Loss: 2.0888 \n",
      "Batch: 49600  Val_Loss: 2.0926 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 49800  Val_Loss: 2.1517 \n",
      "Batch: 50000  Val_Loss: 2.0421 \n"
     ]
    }
   ],
   "source": [
    "model.train(batches, num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from char_rnn/model-50000\n",
      "\n",
      "The wearthis both as, wil him; some to a thing,Te take the sear in the store of some hiss,What tore me a tran on my fingues, and his mean to so thou will not takine, then, thou wast thy to a sore to at thy san other, to he thou wast, the wittince, andestrung of a sent of the seleanch if him, I am, which a douth to have make me should tear this ways of my satistons and more steet of his, that hence that hast to the that the sheeper should nistring thee seeds,With the time to save thy store of him, I shall true to-mister some sometames.PORELILK:And thou wert, this is men in thee it where here say,This.SRORCET:Thou wald to the month, see, more will.Then, allow the man to a sarten and say,And the mistress of he shall be to her.PRESSIDE:If take a dost watch. What to thee, and that is a colsting,Ald my friir than more armiced of the streeds in angle. What's the serven arm of my latt that with the hate to this wind. I will bat thee is a door the hing within them the shit to have my budy, when how sout they will to set that.PROSPE HENRY:I speak as the san, as you shill to have hath should not have te heart to her the streikends of the band, and what to have a say he starse the stangers and his better horre we assains,I said him, but shall but a more a mistress are the better,And sair, have a stone, when his solder to the season.This is hene all a servent soul, I should thoughf hear my stoles of me, to meroul the botter someshere in mights, with all any sain to say teart, the mean mat at the hard with his best: the hour of my father of this. With some maschation of thee, a measure; besieed me and so men.SARTTEN:I have ships and and that we so that the why words. And, with the stind and how here; and with his bory, and terpers, and with this wilt on the banding and their masters, more weaked.SINET:To true and the more.TATSAR:And then.CORTES:It will therefore honge make me?ARIAL:Shord the constanteds. Which says me and say all your bens,If the these would to hath house, when is h\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('char_rnn/')\n",
    "model = Model(1, 1, loader.vocab_size, lr, training=False)\n",
    "model.load(checkpoint)\n",
    "text = model.sample(char, loader.vocab)\n",
    "print()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
